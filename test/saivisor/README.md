# saivisor
This directory contains tools based on eBPF probes, which trace SAI API calls and perform measurements such as functional latency histograms. The primary componenet is the [saivisor.py](saivisor.py) program. Data may be queried or emitted in several forms such as Prometheus time-series database entries, which can be viewed in Grafana dashboards. A built-in gRPC server allows clients to programatically install and query probes,supporting test automation. 

# TODO
* Make separate dockers for build env and tools, and runtime components
# Design and architecture
See the figure below.

![saivisor-arch](images/saivisor-arch.svg)
# Quick-Start - SONiC-DASH Use-case
This describes how to use `saivisor` in the [SONiC-DASH](https://github.com/sonic-net/DASH) project, specifically the bmv2-based soft switch. Future work will generalize it to other targets and platforms which use SAI, such as SONiC whitebox switches.

## Clone and build SONiC-DASH
```
git clone https://github.com/sonic-net/DASH
make clean
make all
```
## Run DASH bmv2 switch and saithrift-server
In two separate consoles:
```
make run-switch           # console 1
make run-saithrift-server # console 2
```
## Run saivisor
This runs saivisor, installs *all* probes found in the running process (i.e. as listed in its object file elf notes section) of the sai-thrift server which is named `saiserver` and starts a prometheus endpoint server to provide metrics data to a polling client.
```
sudo ./saivisor.py --output prometheus
```

See the examples for selecting specific probes, etc. via `./saivispor.py -h`
## Generate some SAI activity
Exercise the target's saithrift interface, the only way to get probe events. One way is to run some test-cases. For example, use another console and execute one or more of the following:
```
make run-saitrift-ptf-tests
make run-saichallenger-functional-tests
make run-saichallenger-scale-tests
make run-saichallenger tests
make run-all-tests
```
### Optional - monitor BPF logging activity
You can monitor logging messages emitted by the kernel routines comprising the eBPF probes. These are emitted by the `printk()` calls inside eBPF code. They are sent to a pseudo-file called `/sys/kernel/debug/tracing/trace_pipe`. This handy script sends this file to a console:
```
./trace_pipe.sh
```
## Run Prometheus Server as Docker container
Run prometheus using a local volume `prom_data` as data storage. You can choose another location.
```
docker run --privileged -u root -d --rm --name prometheus -p 9090:9090 -v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml -v $(pwd)/prom_data:/prometheus prom/prometheus
```

Omitting the `-u root` can cause permissions issues, Prometheus will abort if it can't write to the volume.

You can use `-it` (interactive terminal) in place of `-d` (detached, or daemon, mode) to watch console/log activity in real time. Alternatively, use docker logs -f prometheus.

An alternative is to create the directory and give "all" write access, e.g.:
```
mkdir prom_data
chmod a+w prom_data
```
However on subsequent runs it might sitll fail.

The above steps will result in a container named `prometheus` which will read the [prometheus.yml](prometheus.yml) file and start polling the localhost via its Docker network address at `http://172.17.0.1:8000/metrics.`

### Optional - Manually query Prometheus metrics server
Use `curl` to query the metrics endpoint. Some metrics are built-in to the Prometheus server (platform and server metrics) and some are generated by saivisor:
```
$ curl localhost:8000/metrics
# HELP python_gc_objects_collected_total Objects collected during gc
# TYPE python_gc_objects_collected_total counter
python_gc_objects_collected_total{generation="0"} 255.0
python_gc_objects_collected_total{generation="1"} 184.0
python_gc_objects_collected_total{generation="2"} 0.0
...
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 196.0
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 1024.0
```
### Optional - Run Prometheus Web Client
Prometheus contains a very rudimentary web client which lets you view time-series data as tables or graphs. Point a web browser to `localhost:9090` and use the pull-down menu to select from the available metrics. Again, until some SAI API calls are made we may not see any saivisor probe metrics, but this is a good way to confirm the Prometheus database is being filled with polled data.

## Run Grafana as a Docker Container

```
docker run -d --rm --name grafana -p 3000:3000 -v grafana_config:/etc/grafana -v grafana_data:/var/lib/grafana -v grafana_logs:/var/log/grafana grafana/grafana
```

* Browse to `localhost:3000`
* Sign in as `admin:admin`
* Add a Prometheus datasource at `http://172.17.0.1:9090` using the Docker IP address. Note, the "datasource UID" is baked into the URL when you're viewing a dashboard. FOr example, the URL `http://localhost:3000/d/ll0AXfPnk/sai-visor?orgId=1&refresh=5s` contains UID `ll0AXfPnk`.
### Dashboard Option #1: Create a Grafana dashboard JSON file, Manually Import In Grafana GUI
This creates a dashboard file for all (as of this writing) dash-related "entry" objects (or "quad" APIs in SONiC syncd parlance). 
```
./saivisor.py --output grafana --quad_probes "outbound_routing_entry,outbound_routing_entry,pa_validation_entry,vip_entry,pa_validation_entry,eni_ether_address_map_entry,outbound_ca_to_pa_entry,outbound_ca_to_pa_entry,direction_lookup_entry,inbound_routing_entry,eni_ether_address_map_entry"   --dash_uid 10 --datasource_uid vf50YxP7z > dashboard_11.json
```

Use Grafana "Dashboard|Import" tool to upload this file and view the dashboard.
### Dashboard Option #2: Create a Grafana dashboard JSON file, Upload to Grafana in one step
This creates the same dashboard as above, but uses an HTTP POST to upload it into the GRafana
